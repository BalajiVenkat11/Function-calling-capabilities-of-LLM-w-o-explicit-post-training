# -*- coding: utf-8 -*-
"""Function Calling benchmark using OSS LLMs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ww_z1kDpmf5USCPDyouPJFVEKFP-qHbJ
"""

## Use one model at a time -- I am not looping through models given colab GPU can handle only ~15G RAM

## Creating and hosting a fake API / Function that I want my LLM to call

from flask import Flask, request, jsonify
from threading import Thread

app = Flask(__name__)

# Full moon gem mapping
moon_gems = {
    "Aries": "Diamond", "Taurus": "Emerald", "Gemini": "Agate",
    "Cancer": "Pearl", "Leo": "Ruby", "Virgo": "Sapphire",
    "Libra": "Opal", "Scorpio": "Topaz", "Sagittarius": "Turquoise",
    "Capricorn": "Garnet", "Aquarius": "Amethyst", "Pisces": "Aquamarine"
}

@app.route('/get_gem', methods=['POST'])
def get_gem():
    data = request.get_json()
    sign = data.get("moon_sign", "").capitalize()
    gem = moon_gems.get(sign, "Unknown")
    return jsonify({"moon_sign": sign, "gem": gem})

# Run Flask in background thread
def run_flask():
    app.run(port=5000)

Thread(target=run_flask).start()

# Prompts
function_prompts = [
    "What is the lucky gem if my moon sign is Aries?",
    "Tell me the gemstone for Libra.",
    "Which gem suits Cancer moon?"
]

normal_prompts = [
    "Who authored the novel Hobbit?"
]

prompts = function_prompts + normal_prompts
print(prompts)

#Model 1--- Tiny LLAMA- 1.1 B params and not function calling trained

!pip install -q transformers accelerate torch

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Use GPU if available
device = "cuda" if torch.cuda.is_available() else "cpu"
print("Using device:", device)

model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,        # use FP16 to save GPU RAM
    device_map="auto",                # automatically place on GPU
    low_cpu_mem_usage=True)

#Model 2- Qwen 2.5 w/ 7B params and post trained on generating structured outputs especially JSON


!pip install -q transformers accelerate torch

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Use GPU if available
device = "cuda" if torch.cuda.is_available() else "cpu"
print("Using device:", device)

# --- CHANGE ONLY THIS MODEL NAME ---
model_name = "Qwen/Qwen2.5-7B-Instruct"
# -----------------------------------

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,        # fp16 for Colab GPU
    device_map="auto",
    trust_remote_code=True,
    low_cpu_mem_usage=True
)

#Model 3- Mistral instruct-v0.2 w/ 7B params and not explicity post trained on JSON out/FC


!pip install -q transformers accelerate torch

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Use GPU if available
device = "cuda" if torch.cuda.is_available() else "cpu"
print("Using device:", device)

# --- CHANGE ONLY THIS MODEL NAME ---
model_name = "mistralai/Mistral-7B-Instruct-v0.2"
# -----------------------------------

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    trust_remote_code=True
)

# Load model
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,      # fp16 for Colab GPU
    device_map="auto",
    trust_remote_code=True,
    low_cpu_mem_usage=True
)

print("Model loaded successfully!")

system_instruction = """
You are an API-wrapper Assistant. You MUST obey the rules below with zero exceptions.

YOUR ONLY TASK:
When the user asks anything related to moon signs, gemstones, lucky stone, zodiac gems, astrological stones, or which gem suits a sign â†’ you MUST output:

{"function":"get_gem","args":{"moon_sign":"<value>"}}

STRICT EXAMPLES (MUST COPY FORMAT EXACTLY):

User: Which gem suits Cancer moon?
Assistant: {"function":"get_gem","args":{"moon_sign":"cancer"}}

User: Lucky stone for Pisces?
Assistant: {"function":"get_gem","args":{"moon_sign":"pisces"}}

User: What stone for SCORPIO moon sign?
Assistant: {"function":"get_gem","args":{"moon_sign":"scorpio"}}

FAILURE MODE:
If you cannot comply 100% with the JSON schema â†’ output NOTHING.

"""

import json
import re
from google.colab import data_table
import pandas as pd
import requests  # make sure requests is imported

rows = []

for user_prompt in prompts:
    full_prompt = f"{system_instruction}\nUser: {user_prompt}\nAssistant:"

    # Generate initial response
    inputs = tokenizer(full_prompt, return_tensors="pt").to(device)
    outputs = model.generate(**inputs, max_new_tokens=50)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # ðŸ”¥ Print initial raw model output
    print("\n--- Initial Model Output ---")
    print(response)
    print("--------------------------------\n")

    # ðŸ§  Extract ONLY the first Assistant JSON block (ignore trailing User:)
    assistant_text = response.split("Assistant:")[-1].strip() if "Assistant:" in response else response.strip()

    # Regex: match JSON containing "function":"get_gem" before any next User: or end-of-string
    json_match = re.search(r'(\{.*?"function"\s*:\s*"get_gem".*?\})(?=\s*User:|$)', assistant_text, re.DOTALL | re.IGNORECASE)

    if not json_match:
        print("No JSON found:", response)
        rows.append([user_prompt, response.strip()])
        continue

    json_str = json_match.group(0)

    # Attempt JSON parsing
    try:
        data = json.loads(json_str)
        if data.get("function") == "get_gem":
            args = data.get("args", {})

            # âœ… Call the Flask API
            api_url = "http://127.0.0.1:5000/get_gem"
            api_response = requests.post(api_url, json=args).json()

            # Feed API response back to LLM for final output (original prompt only)
            followup_input = f"User: {user_prompt}\nAPI Response: {api_response}\nAssistant:"
            followup_inputs = tokenizer(followup_input, return_tensors="pt").to(device)
            final_output = model.generate(**followup_inputs, max_new_tokens=30)
            final_response = tokenizer.decode(final_output[0], skip_special_tokens=True)

            # ðŸ”¹ Clean final response
            print(final_response.strip())
            rows.append([user_prompt, final_response.strip()])
        else:
            print("Function not recognized, printing raw response")
            rows.append([user_prompt, response.strip()])
    except json.JSONDecodeError:
        print("JSON decode error, printing raw response")
        rows.append([user_prompt, response.strip()])

# Display as interactive Google Colab table
df = pd.DataFrame(rows, columns=["Prompt", "Final Response"])
data_table.enable_dataframe_formatter()
df